\chapter{Introducción}

Todas las organizaciones requieren un conjunto de normas para funcionar adecuadamente,
ya sea que éstas se den a conocer de forma verbal o que estén redactadas
en documentos, deben cumplir el propósito de establecer funciones, aplicar
reglas y, en general, regular el comportamiento de los individuos que
conforman la organización.

Las organizaciones constituidas formalmente, y especialmente aquellas
de gran tamaño, poseen una normativa redactada en documentos
de texto que suelen ser extensos y contener un
lenguaje que no es familiar para todos los miembros, lo cual puede dificultar el
entendimiento completo de las normas. Por otro lado, en la última década, se han presentado avances significativos
en lo que popularmente se conoce como \textit{Inteligencia Artificial}, tratándose
específicamente de una mejora notable en los modelos de lenguaje, que actualmente
permiten a los sistemas computacionales comunicarse con los usuarios empleando
lenguaje natural y desempeñar tareas relacionadas con la interpretación de texto.

Con el objetivo de aprovechar estas herramientas y favorecer a la comunidad
universitaria, surge el propósito de generar una herramienta que ayude a conocer
y entender la normativa de la Universidad de Guanajuato. Con este fin, se emplearán
técnicas modernas de procesamiento de texto para crear un asistente virtual,
tipo chatbot, que tenga la capacidad de resolver dudas acerca de los
documentos normativos de la universidad de forma fundamentada, y así, evitar
la vulneración de los derechos de la comunidad y evitar omisiones en el
cumplimiento de sus obligaciones.

\section{Justificación}

La normatividad vigente de la Universidad de Guanajuato (UG) se puede consultar
en su página oficial \footnote{https://www.ugto.mx/}, en la cual se presentan
22 documentos de texto en formato PDF que, en conjunto, pesan 6.9 MB. Si
se convierten estos documentos a texto plano, se obtienen 1.1 MB de información,
lo que equivale a aproximadamente 168,000 palabras. Además, la página web presenta otros documentos
relevantes como el Plan de Desarrollo Institucional, la Gaceta Universitaria,
acuerdos y actas de sus distintos órganos de gobierto, entre otros,
los cuales abonan a la cantidad de textos que se espera que alumnos,
docentes y administrativos conozcan y den seguimiento a sus actualizaciones.

Sin embargo, una dificultad que se presenta al momento de divulgar documentos de carácter
oficial es la falta de cultura lectora en la sociedad mexicana. Según
datos del MOLEC 2024 (Módulo de lectura del INEGI) \cite{inegi_modulo_2024},
el porcentaje de población lectora disminuyó 14.6 puntos porcentuales en los últimos
nueve años, pasando del 84.2 \% en 2015 al 69.6 \% en 2024. Aunque se observa
una leve mejora de la cantidad de población lectora con respecto a 2023 (68.5 \%),
la falta de hábitos de lectura en la población es notable. De aquí se
presenta la necesidad de proponer estrategias alternativas en la divulgación de documentos
normativos, como puede ser el uso de asistentes inteligentes.

Por otra parte, el Foro Económico Mundial 2020 (WEF) presentó el concepto de
Educación 4.0 \cite{world_economic_forum_schools_2020}, en la cual se reconoce que la
educación debe adaptarse a las necesidades del futuro y para ello el UNESCO-UNEVOC
(Centro Internacional para la Educación y Formación Técnica y Profesional)
define la Educación 4.0 como una técnica de aprendizaje que se centra en
transformar la educación con el uso de tecnologías avanzadas, reconociendo la
Inteligencia Artificial como una de ellas. Alineado con esta visión, este proyecto
pretende servir como ejemplo de la integración de la inteligencia artificial en
el ámbito educativo, para resolver una de las necesidades de la comunidad. Así
mismo, ayudará a fomentar el uso responsable de las tecnologías como los chatbots
inteligentes al emplearlos como herramientas de apoyo en la interpretación de
documentos normativos.

Esta tendencia del uso de la tecnología en el ámbito educativo se
intensificó con la introducción de ChatGPT en 2022 \cite{openai_introducing_2022},
y los denominados modelos de lenguaje con capacidades conversacionales o chatbots inteligentes.
Estas herramientas se han integrado en diversos ámbitos de la vida cotidiana,
siendo la educación una de las más influenciadas, motivando a los investigadores
a analizar el impacto que tienen estas herramientas en la educación, como es el
caso de Peláez-Sánchez et al. \cite{pelaez-sanchez_impact_2024}.
Así mismo, se introduce la incógnita qué usos se les puede dar a estas herramientas en
beneficio de la comunidad académica, siendo ésta la motivación del presente proyecto.

Si bien en el mercado actual existen chatbots inteligentes capaces de
responder preguntas relacionadas con documentos que les sean proporcionados, resulta
difícil para los usuarios proporcionarles todo el contexto necesario para que sus respuestas sean
correctas, provengan de fuentes autorizadas y estén debidamente
referenciadas. Además, estas herramientas comerciales presentan desventajas
marcadas, como el hecho de que suelen estar sujetas a políticas de uso o privacidad que pueden
ser desfavorables para los usuarios; pueden presentar variaciones
en su costo, su nivel de personalización es limitado y, generalmente, se desconoce
el uso que se da a los datos confidenciales que se comparten en ellas. Lo anterior
obliga a considerar opciones de código abierto, con las cuales es posible alcanzar
un mayor nivel de personalización, contar con un mejor control sobre las políticas de privacidad
y garantizar la continuidad del sistema mediante un plan adecuado de mantenimiento
y actualización. Es bajo esta filosofía que este proyecto pretende crear un sistema
adaptado a las necesidades de los usuarios y que funcione con herramientas de código
abierto.

\section{Antecedentes}

El Procesamiento de Lenguaje Natural (NLP, Natural Language Processing) es una rama
de la inteligencia artificial y la lingüística, dedicada a dotar a las computadoras
de entendimiento de frases o  palabras escritas en lenguajes humanos \cite{khurana_natural_2023}.
En este proyecto, intervienen diferentes áreas del NLP, como el modelado del lenguaje
(LM, Language Modeling), la respuesta a preguntas (QA, Question Answering) y la
similitud semántica de texto (STS, Semantic Textual Similarity), por mencionar
algunas.

Respecto al modelado de lenguaje, es el área del NLP que busca predecir la siguiente palabra o carácter en una
secuencia de texto dada. En esta área se han presentado avances significativos en las últimas décadas,
comenzando por el desarrollo del modelo neuronal probabilístico de Bengio et al. \cite{bengio_neural_2003},
que empleaba una red neuronal y una tabla de búsqueda para predecir el siguiente carácter.
Posteriormente, Collobert \& Weston \cite{collobert_unified_2008} propusieron
el uso de redes neuronales entrenadas de forma semisupervisada para aprendizaje
multitarea. Más adelante, Mikolov et al. \cite{mikolov_distributed_2013} introdujeron los
\textit{embeddings} para aprender representaciones vectoriales distribuidas
de las palabras. Luego surgieron los modelos \textit{secuencia a secuencia} que utilizaban memoria a
largo corto-plazo (LSTM) para convertir una secuencia de texto en otra, desarrollados por
Sutskever et al. \cite{sutskever_sequence_2014}. A su vez, Bahdanau et al. \cite{bahdanau_neural_2016}
presentaron la mejora de los modelos \textit{codificador-decodificador} empleando mecanismos de atención para
detectar partes de la oración relevantes. Finalmente, Vaswani et al. \cite{vaswani_attention_2017}
introdujeron los \textit{transformers}, una arquitectura de red neuronal basada únicamente en mecanismos de atención,
que alcanzó excelentes resultados en tareas de traducción de texto automática.

La creación de los \textit{transformers} dio origen a los Modelos de Lenguaje de Gran Tamaño
(LLM, Large Language Models), los cuales emplean técnicas de aprendizaje profundo,
particularmente arquitecturas basadas en \textit{transformers}, para aprender y entender
patrones complejos y estructuras presentes en los datos. A su vez, la mejora
de los LLMs llevó a la creación del Transformer Generativamente Pre-entrenado
(GPT) por Radford et al. \cite{radford_improving_2018}, una arquitectura basada en
\textit{transformers}, acompañada de un método de entrenamiento que consiste en pre-entrenar un modelo de
lenguaje para predecir la siguiente palabra, empleando un corpus de texto
de gran tamaño sin etiquetar, seguido de un ajuste fino en tareas
específicas. Con esta técnica, fue posible alcanzar resultados superiores
al estado del arte en diversas tareas de NLP, como son la inferencia de
lenguaje natural, dar respuesta a preguntas, búsquedas por similitud
semántica, entre otras.

Versiones posteriores del modelo GPT incluyeron conjuntos de
entrenamiento más extensos, cambios en las técnicas de ajuste fino, incremento en
el tamaño del modelo, entre otras mejoras, dando como resultado el lanzamiento de
forma comercial de ChatGPT-3 \cite{openai_introducing_2022}. Este modelo se definió como un modelo que interactúa de forma
conversacional, con capacidad de responder preguntas de seguimiento, admitir sus
errores, objetar premisas incorrectas y rechazar peticiones inapropiadas. Tras
la liberación de ChatGPT-3 al público, múltiples empresas y organizaciones
comenzaron a hacer uso de metodologías de entrenamiento similares para
desarrollar sus propios modelos conversacionales y competir en el mercado.

Estos modelos conversacionales se consideran chatbots, los cuales se
definen como sistemas computacionales inteligentes con capacidades de conversación,
que son diseñados para emular una conversación humana con el objetivo de
proporcionar orientación o apoyo \cite{caldarini_literature_2022}.

En la actualidad, los modelos generativos basados en transformers como Chat GPT-4
(OpenAI), Gemini (Google), Llama (Meta), DeepSeek (DeepSeek), entre otros, son el
estado del arte en cuanto a chatbots multidominio, ya que pueden responder preguntas
y realizar tareas en diferentes dominios de conocimiento, siendo algunos de ellos
también multimodales. Estos modelos cuentan con varias versiones y tamaños, para
ajustarse a las necesidades de sus usuarios, algunos siendo de paga y otros de
código abierto.

En el ámbito educativo los LLMs han tenido un impacto significativo,
gracias a sus capacidades de generar resúmenes, resaltar partes
importantes de textos, apoyar en tareas de escritura y en general proveer información
a los estudiantes de temas específicos; además de servir a los profesores a generar
material de apoyo personalizado, ayudar en la planeación de las lecciones o para
calificar pruebas de forma semiautomática \cite{kasneci_chatgpt_2023}.

En el ámbito legal se ha explorado el uso de LLMs para apoyar a profesionales
en ley, impuestos y finanzas, como es el caso de la plataforma Harvey.
Esta empresa, a través de una alianza con OpenAI, entrenó un LLM con conocimiento
legal e historial de casos reales, para generar una herramienta tipo chatbot
que puede contestar preguntas teóricas, citar eventos reales y, en general,
funcionar como asistente en la integración y revisión
de casos complejos \cite{openai_customizing_2024}.

En cuanto a otras áreas del NLP, la presentación del \textit{transformer}
derivó en la creación de arquitecturas como BERT (Bidirectional Encoder
Representations from Transformers), propuesta por Devlin et al.
\cite{devlin_bert_2019}. Esta arquitectura tiene la capacidad de generar representaciones
bidireccionales profundas de texto (\textit{embeddings}) con el objetivo de emplearlas posteriormente en un
proceso de ajuste fino (\textit{fine-tuning}) para realizar tareas específicas, alcanzando resultados
superiores al estado del arte en actividades como la similitud semántica de texto.
Posteriormente, se presentaron diversas modificaciones y alternativas a esta arquitectura, entre
las cuales destacan el modelo SBERT (Sentence BERT) de Riemers \& Gurevych
\cite{reimers_sentence-bert_2019}, MiniLM de Wang et al. \cite{wang_minilm_2020}
y Qwen3 de Zhang et al. \cite{zhang_qwen3_2025}, todas con el objetivo de desarrollarse
adecuadamente en tareas de similitud semántica de texto, respuesta a preguntas, entre otras.

Estos modelos de \textit{embeddings}, así como los modelos generativos basados en \textit{transformers},
hacen uso de diferentes técnicas de entrenamiento y ajuste
para obtener los resultados esperados en tareas específicas, siendo las técnicas
más comunes la ingeniería de \textit{prompts} y el ajuste fino.

La ingeniería de \textit{prompts} tiene por objetivo modificar la forma en que un modelo
emite su respuesta, empleando instrucciones que se le proporcionan
en forma de texto. Un ejemplo de aplicación de esta técnica se puede ver en el
trabajo de Patil et al. \cite{patil_prompt_2024} que usó instrucciones para
guiar a los modelos a generar explicaciones amigables para los pacientes sobre
conceptos médicos, enfermedades y opciones de tratamiento, con el fin de
reducir la brecha de conocimientos entre médicos y pacientes.

Por otra parte, el ajuste fino es un proceso en el que un modelo pre-entrenado,
como un LLM, es reentrenado en un conjunto de datos específico para adaptarlo a
tareas o dominios especializados. Dentro de esta metodología existen dos técnicas
ampliamente usadas: Supervised fine-tuning y Reinforcement Learning From Human
Feedback \cite{anisuzzaman_fine-tuning_2025}. Ambas metodologías emplean técnicas
para ajustar el comportamiento del modelo a casos específicos de uso y dotarlo de
comportamientos enfocados en tareas específicas.

A pesar de todas estas cualidades, los modelos de lenguaje usualmente enfrentan problemas como las
alucinaciones, la desactualización del conocimiento y la falta de transparencia y
trazabilidad de su proceso de razonamiento. Con el objetivo de aliviar estos
problemas se desarrollaron las técnicas de recuperación-generación aumentada de
información o generación aumentada por recuperación (RAG, Retrieval-Augmented
Generation). Las técnicas RAG consisten en mejorar los LLMs al extraer
fragmentos de información relevante de bases de conocimiento externas, a través
de cálculos de similitud semántica.
Por ejemplo, en una investigación realizada en el ámbito de la medicina, se
encontró que el uso de RAGs en conjunto con LLMs puede mejorar hasta un 39.7 \%
la exactitud en las respuestas de preguntas relacionadas a las normas de la
American Academy of Orthopaedic Surgeons (AAOS), para la atención a lesiones
del ligamento cruzado anterior \cite{woo_custom_2025}.

\section{Objetivos}

\subsection{Objetivo General}

Desarrollar un asistente virtual tipo chatbot, alojado en la nube, que responda
preguntas sobre la normativa de la Universidad de Guanajuato, implementando cuatro
flujos de trabajo: extracción de información con RAGs, reentrenamiento de LLMs,
backend y frontend. Los cuatro flujos de trabajo emplearán técnicas y herramientas
de CI/CD y MLOps o DevOps (según sea el caso) para el despliegue continuo y
actualización del sistema.

\subsection{Objetivos específicos}

\begin{itemize}
      \item Implementar un algoritmo de extracción de información de documentos
            normativos de la Universidad de Guanajuato basado en técnicas RAG, que
            pueda interactuar con un LLM.
      \item Reentrenar un LLM que se comunique con el algoritmo de extracción de
            información, para que sea capaz de interpretar la normativa de la Universidad
            de Guanajuato y contestar preguntas relacionadas de forma especializada y
            personalizada, proporcionando referencias de dónde se obtiene la información.
      \item Desarrollar una API, empleando FastAPI o una tecnología similar,
            para comunicar una aplicación cliente con el modelo.
      \item Desarrollar una aplicación web tipo chatbot, empleando React o una
            tecnología similar, para que un usuario pueda hacer consultas al modelo a
            través de internet.
      \item Implementar un flujo de trabajo para el algoritmo de extracción de
            información que permita la actualización automática o semisupervisada de
            la nueva normativa.
      \item Implementar un flujo de trabajo, utilizando herramientas de MLOps,
            para el reentrenamiento, actualización y despliegue del LLM de forma automática
            o semisupervisada.
      \item Implementar un flujo de trabajo para la API y uno para la aplicación
            web, que emplee técnicas y herramientas de CI/CD y DevOps para desplegar
            actualizaciones de forma automática o semisupervisada.
\end{itemize}

\section{Hipótesis del trabajo}

Es posible emplear recuperación-generación aumentada de información con información
de un dominio específico y limitado, así como reentrenar un modelo de lenguaje
para que pueda proporcionar información y resolver dudas que sean entendibles
por personas sin conocimiento previo. La información que se emplee puede
actualizarse con documentos recientes y el modelo puede reentrenarse para mejorar
la calidad de las respuestas basándose en la interacción con los usuarios. Además,
esta metodología puede aplicarse con diferentes fuentes de información, siempre
que se ajuste el modelo al ámbito deseado.

\section{Organización de la tesis}

Este documento se divide en 5 capítulos. Esta introducción tiene el propósito
de enfatizar los beneficios y cualidades de los modelos de lenguaje como
herramientas multidominio y, específicamente, para responder preguntas e
interpretar documentos. En el capítulo dos se presentan los conceptos
esenciales que se requiere entender para realizar el proyecto, así como
las herramientas de las que se hará uso. En el capítulo tres se presenta
la metodología a seguir para alcanzar el objetivo de generar y poner en
marcha la herramienta que sea capaz de responder preguntas con las características
deseadas. Posteriormente, en el capítulo cuatro se muestran
los resultados de implementación de la metodología así como el análisis
de los mismos. Finalmente, en el último capítulo se presentan las conclusiones
finales del proyecto.