
@misc{inegi_modulo_2024,
  type       = {Press release},
  title      = {Módulo sobre {Lectura} ({MOLEC}) 2024},
  shorttitle = {{MOLEC}},
  url        = {https://www.inegi.org.mx/contenidos/saladeprensa/boletines/2024/molec/molec2024.pdf},
  language   = {es},
  urldate    = {0225-01-27},
  journal    = {INEGI Sala de prensa},
  author     = {{INEGI}},
  month      = apr,
  year       = {2024},
  file       = {PDF:/home/rgarcia/Zotero/storage/98JG72P5/2024 - Módulo sobre Lectura (MOLEC) 2024.pdf:application/pdf}
}

@misc{openai_introducing_2022,
  type       = {Press release},
  title      = {Introducing {ChatGPT}},
  shorttitle = {Introducing {ChatGPT}},
  url        = {https://openai.com/index/chatgpt/},
  abstract   = {We’ve trained a model called ChatGPT which interacts in a conversational way. The dialogue format makes it possible for ChatGPT to answer followup questions, admit its mistakes, challenge incorrect premises, and reject inappropriate requests.},
  language   = {en-US},
  urldate    = {2025-01-27},
  journal    = {OpenAI},
  author     = {{OpenAI}},
  month      = nov,
  year       = {2022},
  file       = {Snapshot:/home/rgarcia/Zotero/storage/3AUVFUK8/chatgpt.html:text/html}
}

@article{pelaez-sanchez_impact_2024,
  title      = {The impact of large language models on higher education: exploring the connection between {AI} and {Education} 4.0},
  volume     = {9},
  issn       = {2504-284X},
  shorttitle = {The impact of large language models on higher education},
  url        = {https://www.frontiersin.orghttps://www.frontiersin.org/journals/education/articles/10.3389/feduc.2024.1392091/full},
  doi        = {10.3389/feduc.2024.1392091},
  abstract   = {The digital transformation has profoundly affected every facet of human life, with technological advancements potentially reshaping the economy, society, and our daily living and working modalities. Artificial Intelligence (AI), particularly Generative AI (GAI), has emerged as a pivotal disruption in education, showcasing the capability to produce diverse and context-relevant content. Generative Artificial Intelligence (GAI) has revolutionized natural language processing, computer vision, and creative arts. Large language models (LLMs) like GPT-4 and Open Assistant and tools like DALL-E and Midjourney for the visual and creative domain are increasingly used for various tasks by students and others with critical information needs. AI presents novel avenues for crafting effective learning activities and developing enhanced technology-driven learning applications in the educational sector. However, integrating AI with a pedagogical focus pose challenge. Education 4.0, which integrates emerging technologies and innovative strategies, aims to prepare new generations for a technologically fluid world. This systematic literature review aims to analyze the use of LLMs in higher education within the context of Education 4.0’s pedagogical approaches, identifying trends and challenges from a selection of 83 relevant articles out of an initial set of 841 papers. The findings underscore the significant potential of LLMs to enrich higher education, aligning with Education 4.0 by fostering more autonomous, collaborative, and interactive learning. It highlights the necessity for human oversight to ensure the quality and accuracy of AI-generated content. It addresses ethical and legal challenges to ensure equitable implementation, suggesting an exploration of LLM integration that complements human interaction while maintaining academic integrity and pedagogical foundation.},
  language   = {English},
  urldate    = {2025-04-28},
  journal    = {Frontiers in Education},
  author     = {Peláez-Sánchez, Iris Cristina and Velarde-Camaqui, Davis and Glasserman-Morales, Leonardo David},
  month      = jun,
  year       = {2024},
  note       = {Publisher: Frontiers},
  keywords   = {AI, Artificia lintelligence, Education 4.0, Generative Artificial Intelligence (GAI), higer education, Innovation in education, large language model (LLM)},
  file       = {Full Text PDF:/home/rgarcia/Zotero/storage/289U6N6J/Peláez-Sánchez et al. - 2024 - The impact of large language models on higher education exploring the connection between AI and Edu.pdf:application/pdf}
}

@article{khurana_natural_2023,
  title      = {Natural language processing: state of the art, current trends and challenges},
  volume     = {82},
  issn       = {1573-7721},
  shorttitle = {Natural language processing},
  url        = {https://doi.org/10.1007/s11042-022-13428-4},
  doi        = {10.1007/s11042-022-13428-4},
  abstract   = {Natural language processing (NLP) has recently gained much attention for representing and analyzing human language computationally. It has spread its applications in various fields such as machine translation, email spam detection, information extraction, summarization, medical, and question answering etc. In this paper, we first distinguish four phases by discussing different levels of NLP and components of Natural Language Generation followed by presenting the history and evolution of NLP. We then discuss in detail the state of the art presenting the various applications of NLP, current trends, and challenges. Finally, we present a discussion on some available datasets, models, and evaluation metrics in NLP.},
  language   = {en},
  number     = {3},
  urldate    = {2025-04-28},
  journal    = {Multimedia Tools and Applications},
  author     = {Khurana, Diksha and Koli, Aditya and Khatter, Kiran and Singh, Sukhdev},
  month      = jan,
  year       = {2023},
  keywords   = {Artificial Intelligence, Natural language generation, Natural language processing, Natural language understanding, NLP applications, NLP evaluation metrics},
  pages      = {3713--3744},
  file       = {Versión enviada:/home/rgarcia/Zotero/storage/SV4D8RE9/Khurana et al. - 2023 - Natural language processing state of the art, current trends and challenges.pdf:application/pdf}
}

@inproceedings{mikolov_distributed_2013,
  title     = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
  volume    = {26},
  url       = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  abstract  = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships.  In this paper we present several improvements that make the Skip-gram model more expressive and enable it to learn higher quality vectors more rapidly.  We show that by subsampling frequent words we obtain significant speedup,  and also learn higher quality representations as measured by our tasks. We also introduce Negative Sampling, a simplified variant of Noise Contrastive Estimation (NCE) that learns more accurate vectors for frequent words compared to the hierarchical softmax.   An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases.  For example, the meanings of Canada'' and "Air'' cannot be easily combined to obtain "Air Canada''.  Motivated by this example, we present a simple and efficient method for finding phrases, and show that their vector representations can be accurately learned by the Skip-gram model. "},
  urldate   = {2025-04-28},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  year      = {2013},
  file      = {Full Text PDF:/home/rgarcia/Zotero/storage/TA3SUNLI/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:application/pdf}
}

@inproceedings{vaswani_attention_2017,
  title     = {Attention is {All} you {Need}},
  volume    = {30},
  url       = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  abstract  = {The dominant sequence transduction models are based on complex recurrent orconvolutional neural networks in an encoder and decoder configuration. The best performing such models also connect the encoder and decoder through an attentionm echanisms.  We propose a novel, simple network architecture based solely onan attention mechanism, dispensing with recurrence and convolutions entirely.Experiments on two machine translation tasks show these models to be superiorin quality while being more parallelizable and requiring significantly less timeto train. Our single model with 165 million parameters, achieves 27.5 BLEU onEnglish-to-German translation, improving over the existing best ensemble result by over 1 BLEU. On English-to-French translation, we outperform the previoussingle state-of-the-art with model by 0.7 BLEU, achieving a BLEU score of 41.1.},
  urldate   = {2025-04-28},
  booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
  publisher = {Curran Associates, Inc.},
  author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Lukasz and Polosukhin, Illia},
  year      = {2017},
  file      = {Full Text PDF:/home/rgarcia/Zotero/storage/CDAPQ5ZJ/Vaswani et al. - 2017 - Attention is All you Need.pdf:application/pdf}
}

@article{radford_improving_2018,
  title    = {Improving {Language} {Understanding} by {Generative} {Pre}-{Training}},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classiﬁcation. Although large unlabeled text corpora are abundant, labeled data for learning these speciﬁc tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative ﬁne-tuning on each speciﬁc task. In contrast to previous approaches, we make use of task-aware input transformations during ﬁne-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures speciﬁcally crafted for each task, signiﬁcantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  language = {en},
  journal  = {OpenAI},
  author   = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  month    = jun,
  year     = {2018},
  file     = {PDF:/home/rgarcia/Zotero/storage/GUT9Q766/Radford et al. - Improving Language Understanding by Generative Pre-Training.pdf:application/pdf}
}

@article{caldarini_literature_2022,
  title     = {A {Literature} {Survey} of {Recent} {Advances} in {Chatbots}},
  volume    = {13},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2078-2489},
  url       = {https://www.mdpi.com/2078-2489/13/1/41},
  doi       = {10.3390/info13010041},
  abstract  = {Chatbots are intelligent conversational computer systems designed to mimic human conversation to enable automated online guidance and support. The increased benefits of chatbots led to their wide adoption by many industries in order to provide virtual assistance to customers. Chatbots utilise methods and algorithms from two Artificial Intelligence domains: Natural Language Processing and Machine Learning. However, there are many challenges and limitations in their application. In this survey we review recent advances on chatbots, where Artificial Intelligence and Natural Language processing are used. We highlight the main challenges and limitations of current work and make recommendations for future research investigation.},
  language  = {en},
  number    = {1},
  urldate   = {2025-04-28},
  journal   = {Information},
  author    = {Caldarini, Guendalina and Jaf, Sardar and McGarry, Kenneth},
  month     = jan,
  year      = {2022},
  note      = {Number: 1
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {chatbot, ChatScript, conversation systems, conversational agents, conversational entities, conversational modelling, conversational system, embodied conversational agents, human-computer dialogue system, social chatbots},
  pages     = {41},
  file      = {Full Text PDF:/home/rgarcia/Zotero/storage/CBJHRJ5Q/Caldarini et al. - 2022 - A Literature Survey of Recent Advances in Chatbots.pdf:application/pdf}
}

@article{kasneci_chatgpt_2023,
  title      = {{ChatGPT} for good? {On} opportunities and challenges of large language models for education},
  volume     = {103},
  issn       = {1041-6080},
  shorttitle = {{ChatGPT} for good?},
  url        = {https://www.sciencedirect.com/science/article/pii/S1041608023000195},
  doi        = {10.1016/j.lindif.2023.102274},
  abstract   = {Large language models represent a significant advancement in the field of AI. The underlying technology is key to further innovations and, despite critical views and even bans within communities and regions, large language models are here to stay. This commentary presents the potential benefits and challenges of educational applications of large language models, from student and teacher perspectives. We briefly discuss the current state of large language models and their applications. We then highlight how these models can be used to create educational content, improve student engagement and interaction, and personalize learning experiences. With regard to challenges, we argue that large language models in education require teachers and learners to develop sets of competencies and literacies necessary to both understand the technology as well as their limitations and unexpected brittleness of such systems. In addition, a clear strategy within educational systems and a clear pedagogical approach with a strong focus on critical thinking and strategies for fact checking are required to integrate and take full advantage of large language models in learning settings and teaching curricula. Other challenges such as the potential bias in the output, the need for continuous human oversight, and the potential for misuse are not unique to the application of AI in education. But we believe that, if handled sensibly, these challenges can offer insights and opportunities in education scenarios to acquaint students early on with potential societal biases, criticalities, and risks of AI applications. We conclude with recommendations for how to address these challenges and ensure that such models are used in a responsible and ethical manner in education.},
  urldate    = {2025-04-28},
  journal    = {Learning and Individual Differences},
  author     = {Kasneci, Enkelejda and Sessler, Kathrin and Küchemann, Stefan and Bannert, Maria and Dementieva, Daryna and Fischer, Frank and Gasser, Urs and Groh, Georg and Günnemann, Stephan and Hüllermeier, Eyke and Krusche, Stephan and Kutyniok, Gitta and Michaeli, Tilman and Nerdel, Claudia and Pfeffer, Jürgen and Poquet, Oleksandra and Sailer, Michael and Schmidt, Albrecht and Seidel, Tina and Stadler, Matthias and Weller, Jochen and Kuhn, Jochen and Kasneci, Gjergji},
  month      = apr,
  year       = {2023},
  keywords   = {Artificial intelligence, Education, Educational technologies, Large language models},
  pages      = {102274},
  file       = {ScienceDirect Snapshot:/home/rgarcia/Zotero/storage/3ZHU2AN6/S1041608023000195.html:text/html;Versión enviada:/home/rgarcia/Zotero/storage/E5LLLBHH/Kasneci et al. - 2023 - ChatGPT for good On opportunities and challenges of large language models for education.pdf:application/pdf}
}

@misc{openai_customizing_2024,
  type       = {Blog},
  title      = {Customizing models for legal professionals},
  shorttitle = {Harvey},
  url        = {https://openai.com/index/harvey/},
  abstract   = {Harvey partners with OpenAI to build a custom-trained model for legal professionals.},
  language   = {en-US},
  urldate    = {2025-02-07},
  journal    = {OpenAI},
  author     = {{OpenAI}},
  month      = apr,
  year       = {2024}
}

@article{patil_prompt_2024,
  title     = {Prompt {Engineering} in {Healthcare}},
  volume    = {13},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  issn      = {2079-9292},
  url       = {https://www.mdpi.com/2079-9292/13/15/2961},
  doi       = {10.3390/electronics13152961},
  abstract  = {The rapid advancements in artificial intelligence, particularly generative AI and large language models, have unlocked new possibilities for revolutionizing healthcare delivery. However, harnessing the full potential of these technologies requires effective prompt engineering—designing and optimizing input prompts to guide AI systems toward generating clinically relevant and accurate outputs. Despite the importance of prompt engineering, medical education has yet to fully incorporate comprehensive training on this critical skill, leading to a knowledge gap among medical clinicians. This article addresses this educational gap by providing an overview of generative AI prompt engineering, its potential applications in primary care medicine, and best practices for its effective implementation. The role of well-crafted prompts in eliciting accurate, relevant, and valuable responses from AI models is discussed, emphasizing the need for prompts grounded in medical knowledge and aligned with evidence-based guidelines. The article explores various applications of prompt engineering in primary care, including enhancing patient–provider communication, streamlining clinical documentation, supporting medical education, and facilitating personalized care and shared decision-making. Incorporating domain-specific knowledge, engaging in iterative refinement and validation of prompts, and addressing ethical considerations and potential biases are highlighted. Embracing prompt engineering as a core competency in medical education will be crucial for successfully adopting and implementing AI technologies in primary care, ultimately leading to improved patient outcomes and enhanced healthcare delivery.},
  language  = {en},
  number    = {15},
  urldate   = {2025-04-28},
  journal   = {Electronics},
  author    = {Patil, Rajvardhan and Heston, Thomas F. and Bhuse, Vijay},
  month     = jan,
  year      = {2024},
  note      = {Number: 15
               Publisher: Multidisciplinary Digital Publishing Institute},
  keywords  = {chatbot, ChatGPT, GPT, healthcare, healthcare providers, PCP, primary care providers, prompt engineering, prompts},
  pages     = {2961},
  file      = {Full Text PDF:/home/rgarcia/Zotero/storage/FE7RPVE9/Patil et al. - 2024 - Prompt Engineering in Healthcare.pdf:application/pdf}
}

@article{anisuzzaman_fine-tuning_2025,
  title    = {Fine-{Tuning} {Large} {Language} {Models} for {Specialized} {Use} {Cases}},
  volume   = {3},
  issn     = {2949-7612},
  doi      = {10.1016/j.mcpdig.2024.11.005},
  abstract = {Large language models (LLMs) are a type of artificial intelligence, which operate by predicting and assembling sequences of words that are statistically likely to follow from a given text input. With this basic ability, LLMs are able to answer complex questions and follow extremely complex instructions. Products created using LLMs such as ChatGPT by OpenAI and Claude by Anthropic have created a huge amount of traction and user engagements and revolutionized the way we interact with technology, bringing a new dimension to human-computer interaction. Fine-tuning is a process in which a pretrained model, such as an LLM, is further trained on a custom data set to adapt it for specialized tasks or domains. In this review, we outline some of the major methodologic approaches and techniques that can be used to fine-tune LLMs for specialized use cases and enumerate the general steps required for carrying out LLM fine-tuning. We then illustrate a few of these methodologic approaches by describing several specific use cases of fine-tuning LLMs across medical subspecialties. Finally, we close with a consideration of some of the benefits and limitations associated with fine-tuning LLMs for specialized use cases, with an emphasis on specific concerns in the field of medicine.},
  language = {eng},
  number   = {1},
  journal  = {Mayo Clinic Proceedings. Digital Health},
  author   = {Anisuzzaman, D. M. and Malins, Jeffrey G. and Friedman, Paul A. and Attia, Zachi I.},
  month    = mar,
  year     = {2025},
  pmid     = {40206998},
  pmcid    = {PMC11976015},
  pages    = {100184}
}

@article{woo_custom_2025,
  title      = {Custom {Large} {Language} {Models} {Improve} {Accuracy}: {Comparing} {Retrieval} {Augmented} {Generation} and {Artificial} {Intelligence} {Agents} to {Noncustom} {Models} for {Evidence}-{Based} {Medicine}},
  volume     = {41},
  issn       = {1526-3231},
  shorttitle = {Custom {Large} {Language} {Models} {Improve} {Accuracy}},
  doi        = {10.1016/j.arthro.2024.10.042},
  abstract   = {PURPOSE: To show the value of custom methods, namely Retrieval Augmented Generation (RAG)-based Large Language Models (LLMs) and Agentic Augmentation, over standard LLMs in delivering accurate information using an anterior cruciate ligament (ACL) injury case.
                METHODS: A set of 100 questions and answers based on the 2022 AAOS ACL guidelines were curated. Closed-source (open AI GPT4/GPT 3.5 and Anthropic's Claude3) and open-source models (LLama3 8b/70b and Mistral 8×7b) were asked questions in base form and again with AAOS guidelines embedded into a RAG system. The top-performing models were further augmented with artificial intelligence (AI) agents and reevaluated. Two fellowship-trained surgeons blindly evaluated the accuracy of the responses of each cohort. Recall-Oriented Understudy of Gisting Evaluation and Metric for Evaluation of Translation with Explicit Ordering scores were calculated to assess semantic similarity in the response.
                RESULTS: All noncustom LLM models started below 60\% accuracy. Applying RAG improved the accuracy of every model by an average 39.7\%. The highest performing model with just RAG was Meta's open-source Llama3 70b (94\%). The highest performing model with RAG and AI agents was Open AI's GPT4 (95\%).
                CONCLUSIONS: RAG improved accuracy by an average of 39.7\%, with the highest accuracy rate of 94\% in the Meta Llama3 70b. Incorporating AI agents into a previously RAG-augmented LLM improved ChatGPT4 accuracy rate to 95\%. Thus, Agentic and RAG augmented LLMs can be accurate liaisons of information, supporting our hypothesis.
                CLINICAL RELEVANCE: Despite literature surrounding the use of LLM in medicine, there has been considerable and appropriate skepticism given the variably accurate response rates. This study establishes the groundwork to identify whether custom modifications to LLMs using RAG and agentic augmentation can better deliver accurate information in orthopaedic care. With this knowledge, online medical information commonly sought in popular LLMs, such as ChatGPT, can be standardized and provide relevant online medical information to better support shared decision making between surgeon and patient.},
  language   = {eng},
  number     = {3},
  journal    = {Arthroscopy: The Journal of Arthroscopic \& Related Surgery: Official Publication of the Arthroscopy Association of North America and the International Arthroscopy Association},
  author     = {Woo, Joshua J. and Yang, Andrew J. and Olsen, Reena J. and Hasan, Sayyida S. and Nawabi, Danyal H. and Nwachukwu, Benedict U. and Williams, Riley J. and Ramkumar, Prem N.},
  month      = mar,
  year       = {2025},
  pmid       = {39521391},
  keywords   = {Artificial Intelligence, Anterior Cruciate Ligament Reconstruction, Evidence-Based Medicine, Humans, Language, Large Language Models},
  pages      = {565--573.e6}
}

@misc{world_economic_forum_schools_2020,
  type       = {Report},
  title      = {Schools of the {Future}: {Defining} {New} {Models} of {Education} for the {Fourth} {Industrial} {Revolution}},
  shorttitle = {Schools of the {Future}},
  url        = {https://www.weforum.org/publications/schools-of-the-future-defining-new-models-of-education-for-the-fourth-industrial-revolution/},
  abstract   = {“Schools of the Future: Defining New Models of Education for the Fourth Industrial Revolution” outlines a new framework for defining quality education in the new economic and social context and shares key features of 16 schools, systems and programmes pioneering the future of education.},
  language   = {en},
  urldate    = {2025-01-28},
  journal    = {World Economic Forum},
  author     = {{World Economic Forum}},
  month      = jan,
  year       = {2020},
  file       = {Snapshot:/home/rgarcia/Zotero/storage/RSP3DE9D/schools-of-the-future-defining-new-models-of-education-for-the-fourth-industrial-revolution.html:text/html}
}

@inproceedings{collobert_unified_2008,
  address    = {New York, NY, USA},
  series     = {{ICML} '08},
  title      = {A unified architecture for natural language processing: deep neural networks with multitask learning},
  isbn       = {978-1-60558-205-4},
  shorttitle = {A unified architecture for natural language processing},
  url        = {https://doi.org/10.1145/1390156.1390177},
  doi        = {10.1145/1390156.1390177},
  abstract   = {We describe a single convolutional neural network architecture that, given a sentence, outputs a host of language processing predictions: part-of-speech tags, chunks, named entity tags, semantic roles, semantically similar words and the likelihood that the sentence makes sense (grammatically and semantically) using a language model. The entire network is trained jointly on all these tasks using weight-sharing, an instance of multitask learning. All the tasks use labeled data except the language model which is learnt from unlabeled text and represents a novel form of semi-supervised learning for the shared tasks. We show how both multitask learning and semi-supervised learning improve the generalization of the shared tasks, resulting in state-of-the-art-performance.},
  urldate    = {2025-04-29},
  booktitle  = {Proceedings of the 25th international conference on {Machine} learning},
  publisher  = {Association for Computing Machinery},
  author     = {Collobert, Ronan and Weston, Jason},
  month      = jul,
  year       = {2008},
  pages      = {160--167}
}

@inproceedings{sutskever_sequence_2014,
  address   = {Cambridge, MA, USA},
  series    = {{NIPS}'14},
  title     = {Sequence to sequence learning with neural networks},
  volume    = {2},
  abstract  = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  urldate   = {2025-04-29},
  booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
  publisher = {MIT Press},
  author    = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
  month     = dec,
  year      = {2014},
  pages     = {3104--3112}
}

@misc{bahdanau_neural_2016,
  title     = {Neural {Machine} {Translation} by {Jointly} {Learning} to {Align} and {Translate}},
  url       = {http://arxiv.org/abs/1409.0473},
  doi       = {10.48550/arXiv.1409.0473},
  abstract  = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  urldate   = {2025-04-29},
  publisher = {arXiv},
  author    = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  month     = may,
  year      = {2016},
  note      = {arXiv:1409.0473 [cs]},
  keywords  = {Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
  annote    = {Comment: Accepted at ICLR 2015 as oral presentation},
  file      = {Preprint PDF:/home/rgarcia/Zotero/storage/3P5XPXJE/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to Align and Translate.pdf:application/pdf;Snapshot:/home/rgarcia/Zotero/storage/7IKS2RHE/1409.html:text/html}
}

@article{bengio_neural_2003,
  title    = {A neural probabilistic language model},
  volume   = {3},
  issn     = {1532-4435},
  url      = {https://dl.acm.org/doi/10.5555/944919.944966},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  number   = {Feb},
  journal  = {Journal of Machine Learning Research},
  author   = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Janvin, Christian},
  month    = mar,
  year     = {2003},
  pages    = {1137--1155},
  file     = {Full Text PDF:/home/rgarcia/Zotero/storage/GAR4PCC7/Bengio et al. - 2003 - A neural probabilistic language model.pdf:application/pdf}
}

@misc{noauthor_que_nodate,
  title      = {¿{Qué} es un {PDF}? {Formato} de documento portátil {\textbar} {Adobe} {Acrobat}},
  shorttitle = {¿{Qué} es un {PDF}?},
  url        = {https://www.adobe.com/mx/acrobat/about-adobe-pdf.html},
  abstract   = {Descubre qué es un archivo PDF y lo que significan las siglas. En Adobe, creamos el formato de documento portátil para ayudar a las personas a conectar a través del intercambio electrónico de documentos.},
  language   = {es-MX},
  urldate    = {2025-06-11},
  file       = {Snapshot:/home/rgarcia/Zotero/storage/G36WV5S4/about-adobe-pdf.html:text/html}
}

@misc{noauthor_standard_2021,
  title    = {The standard for {PDF} is revised},
  url      = {https://www.iso.org/news/ref2608.html},
  abstract = {Ensuring the world’s most popular document format is as relevant as ever.},
  language = {es},
  urldate  = {2025-06-11},
  journal  = {ISO},
  month    = jan,
  year     = {2021}
}

@inproceedings{devlin_bert_2019,
  address    = {Minneapolis, Minnesota},
  title      = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
  shorttitle = {{BERT}},
  url        = {https://aclanthology.org/N19-1423/},
  doi        = {10.18653/v1/N19-1423},
  abstract   = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  urldate    = {2025-08-26},
  booktitle  = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
  publisher  = {Association for Computational Linguistics},
  author     = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  editor     = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  month      = jun,
  year       = {2019},
  pages      = {4171--4186},
  file       = {Full Text PDF:/home/rgarcia/Zotero/storage/P4UYB4B6/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transformers for Language Understanding.pdf:application/pdf}
}

@inproceedings{reimers_sentence-bert_2019,
  address    = {Hong Kong, China},
  title      = {Sentence-{BERT}: {Sentence} {Embeddings} using {Siamese} {BERT}-{Networks}},
  shorttitle = {Sentence-{BERT}},
  url        = {https://aclanthology.org/D19-1410/},
  doi        = {10.18653/v1/D19-1410},
  abstract   = {BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations ({\textbackslash}textasciitilde65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods.},
  urldate    = {2025-08-26},
  booktitle  = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
  publisher  = {Association for Computational Linguistics},
  author     = {Reimers, Nils and Gurevych, Iryna},
  editor     = {Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun},
  month      = nov,
  year       = {2019},
  pages      = {3982--3992},
  file       = {Full Text PDF:/home/rgarcia/Zotero/storage/E8T62YNL/Reimers y Gurevych - 2019 - Sentence-BERT Sentence Embeddings using Siamese BERT-Networks.pdf:application/pdf}
}

@inproceedings{wang_minilm_2020,
  address    = {Red Hook, NY, USA},
  series     = {{NIPS} '20},
  title      = {{MINILM}: deep self-attention distillation for task-agnostic compression of pre-trained transformers},
  isbn       = {978-1-7138-2954-6},
  shorttitle = {{MINILM}},
  abstract   = {Pre-trained language models (e.g., BERT [12] and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer [42] based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant [26] also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99\% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50\% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.},
  urldate    = {2025-08-26},
  booktitle  = {Proceedings of the 34th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
  publisher  = {Curran Associates Inc.},
  author     = {Wang, Wenhui and Wei, Furu and Dong, Li and Bao, Hangbo and Yang, Nan and Zhou, Ming},
  month      = dec,
  year       = {2020},
  pages      = {5776--5788},
  file       = {Full Text PDF:/home/rgarcia/Zotero/storage/VKML8SNI/Wang et al. - 2020 - MINILM deep self-attention distillation for task-agnostic compression of pre-trained transformers.pdf:application/pdf}
}

@misc{zhang_qwen3_2025,
  title      = {Qwen3 {Embedding}: {Advancing} {Text} {Embedding} and {Reranking} {Through} {Foundation} {Models}},
  shorttitle = {Qwen3 {Embedding}},
  url        = {http://arxiv.org/abs/2506.05176},
  doi        = {10.48550/arXiv.2506.05176},
  abstract   = {In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.},
  urldate    = {2025-08-26},
  publisher  = {arXiv},
  author     = {Zhang, Yanzhao and Li, Mingxin and Long, Dingkun and Zhang, Xin and Lin, Huan and Yang, Baosong and Xie, Pengjun and Yang, An and Liu, Dayiheng and Lin, Junyang and Huang, Fei and Zhou, Jingren},
  month      = jun,
  year       = {2025},
  note       = {arXiv:2506.05176 [cs]},
  keywords   = {Computer Science - Computation and Language},
  file       = {Full Text PDF:/home/rgarcia/Zotero/storage/7SAAKSDP/Zhang et al. - 2025 - Qwen3 Embedding Advancing Text Embedding and Reranking Through Foundation Models.pdf:application/pdf;Snapshot:/home/rgarcia/Zotero/storage/NYHGVE3Z/2506.html:text/html}
}
